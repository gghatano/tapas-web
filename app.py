import streamlit as st
import sys
from pathlib import Path
import pandas as pd

# TAPASãƒ‘ã‚¹ã®è¿½åŠ 
tapas_path = Path(__file__).parent / "tapas"
if str(tapas_path) not in sys.path:
    sys.path.insert(0, str(tapas_path))

st.set_page_config(
    page_title="TAPAS Privacy Evaluation",
    page_icon="ğŸ”’",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ğŸ”’ TAPAS Privacy Evaluation Tool")
st.write("""
TAPAS (Toolbox for Adversarial Privacy Auditing of Synthetic Data) ã¯ã€
åˆæˆãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ãƒ¬ãƒ™ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
""")

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’æŠ˜ã‚ŠãŸãŸã¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«
with st.expander("ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±", expanded=False):
    st.write(f"Python version: {sys.version}")
    st.write(f"Streamlit version: {st.__version__}")
    
    # TAPASã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
    try:
        import tapas
        st.success("âœ… TAPASãŒæ­£å¸¸ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã—ãŸï¼")
        st.write(f"TAPASãƒãƒ¼ã‚¸ãƒ§ãƒ³: {tapas.__version__}")
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ç¢ºèª
        modules = []
        for module in ['datasets', 'generators', 'threat_models', 'attacks', 'report']:
            try:
                __import__(f'tapas.{module}')
                modules.append(module)
            except ImportError:
                pass
        
        if modules:
            st.write("åˆ©ç”¨å¯èƒ½ãªTAPASãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼š")
            st.write(modules)
    except ImportError as e:
        st.warning(f"TAPASã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
st.header("ğŸš€ ã¯ã˜ã‚ã«")

col1, col2 = st.columns(2)

with col1:
    st.subheader("ğŸ“Š ä¸»ãªæ©Ÿèƒ½")
    st.write("""
    - **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: CSVå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œ
    - **ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼æ”»æ’ƒã®ãƒ†ã‚¹ãƒˆ**: å„ç¨®æ”»æ’ƒæ‰‹æ³•ã§ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡
    - **ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ**: è©•ä¾¡çµæœã®å¯è¦–åŒ–ã¨ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    """)

with col2:
    st.subheader("ğŸ” å¯¾å¿œã™ã‚‹æ”»æ’ƒæ‰‹æ³•")
    st.write("""
    - Membership Inference Attack (MIA)
    - Attribute Inference Attack (AIA)
    - Groundhog Attack
    - Closest Distance Attack
    - ãã®ä»–ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼æ”»æ’ƒ
    """)

st.divider()

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³
st.header("ğŸ§ª ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å®Ÿé¨“")

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
dataset_option = st.selectbox(
    "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é¸æŠ",
    ["ãªã—", "UCI Adult Dataset", "ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"]
)

if dataset_option == "UCI Adult Dataset":
    st.subheader("UCI Adult Dataset")
    st.write("""
    UCI Machine Learning Repositoryã®Adult datasetã‚’ä½¿ç”¨ã—ãŸå®Ÿé¨“ãŒå¯èƒ½ã§ã™ã€‚
    ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€äººå£çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦å¹´åãŒ50Kä»¥ä¸Šã‹ã©ã†ã‹ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚
    """)
    
    if st.button("Adult Datasetã‚’ãƒ­ãƒ¼ãƒ‰", key="load_adult"):
        try:
            from ucimlrepo import fetch_ucirepo
            
            with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å–å¾—
                adult = fetch_ucirepo(id=2)
                X = adult.data.features
                y = adult.data.targets
                
                # ãƒ‡ãƒ¼ã‚¿ã®çµåˆ
                data = pd.concat([X, y], axis=1)
                
                # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                DATA_DIR = Path("data/uploaded/adult_dataset")
                DATA_DIR.mkdir(parents=True, exist_ok=True)
                
                csv_path = DATA_DIR / "adult_dataset.csv"
                data.to_csv(csv_path, index=False)
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
                metadata = {
                    "name": "adult_dataset",
                    "description": "UCI Adult Dataset - Income prediction dataset",
                    "original_filename": "adult_uci.csv",
                    "rows": len(data),
                    "columns": len(data.columns),
                    "column_names": data.columns.tolist(),
                    "dtypes": data.dtypes.astype(str).to_dict(),
                    "upload_date": pd.Timestamp.now().isoformat()
                }
                
                metadata_path = DATA_DIR / "metadata.json"
                import json
                with open(metadata_path, "w") as f:
                    json.dump(metadata, f, indent=2)
                
                st.success("âœ… Adult DatasetãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸï¼")
                
                # ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦è¡¨ç¤º
                st.write("### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", len(data))
                with col2:
                    st.metric("ç‰¹å¾´é‡æ•°", len(X.columns))
                with col3:
                    st.metric("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°", y.columns[0])
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                st.write("### ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                st.dataframe(data.head(10))
                
                # çµ±è¨ˆæƒ…å ±
                st.write("### åŸºæœ¬çµ±è¨ˆé‡")
                st.dataframe(data.describe())
                
                st.info("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†ãƒšãƒ¼ã‚¸ã§ã€ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèªã§ãã¾ã™ã€‚")
                
        except ImportError:
            st.error("ucimlrepoãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š")
            st.code("poetry add ucimlrepo")
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

elif dataset_option == "ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ":
    st.subheader("ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ")
    st.write("ç°¡å˜ãªåˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦å®Ÿé¨“ã§ãã¾ã™ã€‚")
    
    col1, col2 = st.columns(2)
    with col1:
        num_records = st.number_input("ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", min_value=100, max_value=10000, value=1000)
        num_features = st.number_input("ç‰¹å¾´é‡æ•°", min_value=2, max_value=20, value=5)
    
    with col2:
        noise_level = st.slider("ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«", min_value=0.0, max_value=1.0, value=0.1)
        random_seed = st.number_input("ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰", min_value=0, value=42)
    
    if st.button("ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ", key="generate_custom"):
        import numpy as np
        
        np.random.seed(random_seed)
        
        # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        X = np.random.randn(num_records, num_features)
        
        # ãƒã‚¤ã‚ºã®è¿½åŠ 
        noise = np.random.randn(num_records, num_features) * noise_level
        X += noise
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ç”Ÿæˆï¼ˆç·šå½¢çµåˆ + ãƒã‚¤ã‚ºï¼‰
        weights = np.random.randn(num_features)
        y = X @ weights + np.random.randn(num_records) * noise_level
        y_binary = (y > np.median(y)).astype(int)
        
        # DataFrameã«å¤‰æ›
        feature_names = [f"feature_{i+1}" for i in range(num_features)]
        data = pd.DataFrame(X, columns=feature_names)
        data['target'] = y_binary
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        DATA_DIR = Path("data/uploaded/custom_dataset")
        DATA_DIR.mkdir(parents=True, exist_ok=True)
        
        csv_path = DATA_DIR / "custom_dataset.csv"
        data.to_csv(csv_path, index=False)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        metadata = {
            "name": "custom_dataset",
            "description": f"Custom generated dataset with {num_records} records and {num_features} features",
            "original_filename": "custom_generated.csv",
            "rows": len(data),
            "columns": len(data.columns),
            "column_names": data.columns.tolist(),
            "dtypes": data.dtypes.astype(str).to_dict(),
            "upload_date": pd.Timestamp.now().isoformat(),
            "generation_params": {
                "num_records": num_records,
                "num_features": num_features,
                "noise_level": noise_level,
                "random_seed": random_seed
            }
        }
        
        metadata_path = DATA_DIR / "metadata.json"
        import json
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2)
        
        st.success("âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        st.write("### ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        st.dataframe(data.head(10))
        
        st.info("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†ãƒšãƒ¼ã‚¸ã§ã€ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèªã§ãã¾ã™ã€‚")

st.divider()

# ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
st.header("ğŸ“ ãƒšãƒ¼ã‚¸ä¸€è¦§")

col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("""
    ### 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†
    
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªã€
    å‰å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚
    
    **â†’ å·¦å´ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰é¸æŠ**
    """)

with col2:
    st.markdown("""
    ### 2. ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼è©•ä¾¡
    
    å„ç¨®æ”»æ’ƒæ‰‹æ³•ã‚’é¸æŠã—ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®
    ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
    
    **â†’ å·¦å´ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰é¸æŠ**
    """)

with col3:
    st.markdown("""
    ### 3. ãƒ¬ãƒãƒ¼ãƒˆ
    
    å®Ÿè¡Œã—ãŸè©•ä¾¡ã®çµæœã‚’ç¢ºèªã—ã€
    ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    
    **â†’ å·¦å´ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰é¸æŠ**
    """)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
st.sidebar.header("ğŸ“Œ ãƒšãƒ¼ã‚¸ä¸€è¦§")
st.sidebar.info("""
æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒåˆ©ç”¨å¯èƒ½ã§ã™ï¼š

- **ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†**: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨ç®¡ç†
- **ğŸ” ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼è©•ä¾¡**: å„ç¨®æ”»æ’ƒã®å®Ÿè¡Œ
- **ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆ**: è©•ä¾¡çµæœã®ç¢ºèªã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

å·¦å´ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ãƒšãƒ¼ã‚¸ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚
""")

st.sidebar.divider()

st.sidebar.header("â„¹ï¸ About")
st.sidebar.info("""
ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã€Alan Turing Instituteã®
TAPASãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åŒ–
ã—ãŸã‚‚ã®ã§ã™ã€‚
""")
